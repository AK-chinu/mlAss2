{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d03c5f-3064-4865-b70d-4f278ddf084a",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to rescale numeric features to a specific range, typically between 0 and 1. It works by subtracting the minimum value of the feature and then dividing by the difference between the maximum and minimum values of the feature.\n",
    "\n",
    "Here's the formula for Min-Max scaling:\n",
    "\n",
    "�\n",
    "scaled\n",
    "=\n",
    "�\n",
    "−\n",
    "�\n",
    "min\n",
    "�\n",
    "max\n",
    "−\n",
    "�\n",
    "min\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "X \n",
    "max\n",
    "​\n",
    " −X \n",
    "min\n",
    "​\n",
    " \n",
    "X−X \n",
    "min\n",
    "​\n",
    " \n",
    "​\n",
    " \n",
    "\n",
    "where:\n",
    "\n",
    "�\n",
    "X is the original value of the feature.\n",
    "�\n",
    "min\n",
    "X \n",
    "min\n",
    "​\n",
    "  is the minimum value of the feature.\n",
    "�\n",
    "max\n",
    "X \n",
    "max\n",
    "​\n",
    "  is the maximum value of the feature.\n",
    "Min-Max scaling ensures that all feature values lie within the range of 0 to 1, preserving the relative differences between values.\n",
    "\n",
    "Example:\n",
    "Let's say we have a dataset containing the following values for a particular feature:\n",
    "\n",
    "Feature\n",
    "=\n",
    "[\n",
    "2\n",
    ",\n",
    "5\n",
    ",\n",
    "10\n",
    ",\n",
    "15\n",
    ",\n",
    "20\n",
    "]\n",
    "Feature=[2,5,10,15,20]\n",
    "\n",
    "To apply Min-Max scaling, we first find the minimum and maximum values of the feature:\n",
    "\n",
    "�\n",
    "min\n",
    "=\n",
    "2\n",
    "X \n",
    "min\n",
    "​\n",
    " =2\n",
    "�\n",
    "max\n",
    "=\n",
    "20\n",
    "X \n",
    "max\n",
    "​\n",
    " =20\n",
    "\n",
    "Now, we can scale each value using the formula:\n",
    "\n",
    "For \n",
    "�\n",
    "=\n",
    "2\n",
    "X=2:\n",
    "�\n",
    "scaled\n",
    "=\n",
    "2\n",
    "−\n",
    "2\n",
    "20\n",
    "−\n",
    "2\n",
    "=\n",
    "0\n",
    "18\n",
    "=\n",
    "0\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "20−2\n",
    "2−2\n",
    "​\n",
    " = \n",
    "18\n",
    "0\n",
    "​\n",
    " =0\n",
    "\n",
    "For \n",
    "�\n",
    "=\n",
    "5\n",
    "X=5:\n",
    "�\n",
    "scaled\n",
    "=\n",
    "5\n",
    "−\n",
    "2\n",
    "20\n",
    "−\n",
    "2\n",
    "=\n",
    "3\n",
    "18\n",
    "=\n",
    "1\n",
    "6\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "20−2\n",
    "5−2\n",
    "​\n",
    " = \n",
    "18\n",
    "3\n",
    "​\n",
    " = \n",
    "6\n",
    "1\n",
    "​\n",
    " \n",
    "\n",
    "For \n",
    "�\n",
    "=\n",
    "10\n",
    "X=10:\n",
    "�\n",
    "scaled\n",
    "=\n",
    "10\n",
    "−\n",
    "2\n",
    "20\n",
    "−\n",
    "2\n",
    "=\n",
    "8\n",
    "18\n",
    "=\n",
    "4\n",
    "9\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "20−2\n",
    "10−2\n",
    "​\n",
    " = \n",
    "18\n",
    "8\n",
    "​\n",
    " = \n",
    "9\n",
    "4\n",
    "​\n",
    " \n",
    "\n",
    "For \n",
    "�\n",
    "=\n",
    "15\n",
    "X=15:\n",
    "�\n",
    "scaled\n",
    "=\n",
    "15\n",
    "−\n",
    "2\n",
    "20\n",
    "−\n",
    "2\n",
    "=\n",
    "13\n",
    "18\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "20−2\n",
    "15−2\n",
    "​\n",
    " = \n",
    "18\n",
    "13\n",
    "​\n",
    " \n",
    "\n",
    "For \n",
    "�\n",
    "=\n",
    "20\n",
    "X=20:\n",
    "�\n",
    "scaled\n",
    "=\n",
    "20\n",
    "−\n",
    "2\n",
    "20\n",
    "−\n",
    "2\n",
    "=\n",
    "18\n",
    "18\n",
    "=\n",
    "1\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "20−2\n",
    "20−2\n",
    "​\n",
    " = \n",
    "18\n",
    "18\n",
    "​\n",
    " =1\n",
    "\n",
    "So, after Min-Max scaling, the feature would be:\n",
    "\n",
    "Feature (scaled)\n",
    "=\n",
    "[\n",
    "0\n",
    ",\n",
    "1\n",
    "6\n",
    ",\n",
    "4\n",
    "9\n",
    ",\n",
    "13\n",
    "18\n",
    ",\n",
    "1\n",
    "]\n",
    "Feature (scaled)=[0, \n",
    "6\n",
    "1\n",
    "​\n",
    " , \n",
    "9\n",
    "4\n",
    "​\n",
    " , \n",
    "18\n",
    "13\n",
    "​\n",
    " ,1]\n",
    "\n",
    "All values are now scaled to a range between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1ac054-c3c4-4f6d-b77f-e194aa173c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09164d07-5be7-4b76-8b67-b69cbda7398f",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as unit normalization or vector normalization, is a feature scaling method used to scale each feature vector to have a length of 1. It differs from Min-Max scaling in that it does not scale the features to a specific range (like 0 to 1), but rather ensures that each feature vector has a magnitude of 1.\n",
    "\n",
    "Here's how the Unit Vector technique works:\n",
    "\n",
    "For each feature vector, calculate its Euclidean length (magnitude).\n",
    "Divide each element of the feature vector by its Euclidean length.\n",
    "This process ensures that each feature vector becomes a unit vector, meaning it lies on the unit hypersphere in the feature space.\n",
    "\n",
    "Example:\n",
    "Let's consider a dataset with two features represented as feature vectors:\n",
    "\n",
    "Feature 1\n",
    "=\n",
    "[\n",
    "3\n",
    ",\n",
    "4\n",
    "]\n",
    "Feature 1=[3,4]\n",
    "Feature 2\n",
    "=\n",
    "[\n",
    "1\n",
    ",\n",
    "2\n",
    "]\n",
    "Feature 2=[1,2]\n",
    "\n",
    "To apply the Unit Vector technique, we follow these steps:\n",
    "\n",
    "Calculate the Euclidean length (magnitude) of each feature vector:\n",
    "Magnitude of Feature 1\n",
    "=\n",
    "3\n",
    "2\n",
    "+\n",
    "4\n",
    "2\n",
    "=\n",
    "9\n",
    "+\n",
    "16\n",
    "=\n",
    "25\n",
    "=\n",
    "5\n",
    "Magnitude of Feature 1= \n",
    "3 \n",
    "2\n",
    " +4 \n",
    "2\n",
    " \n",
    "​\n",
    " = \n",
    "9+16\n",
    "​\n",
    " = \n",
    "25\n",
    "​\n",
    " =5\n",
    "\n",
    "Magnitude of Feature 2\n",
    "=\n",
    "1\n",
    "2\n",
    "+\n",
    "2\n",
    "2\n",
    "=\n",
    "1\n",
    "+\n",
    "4\n",
    "=\n",
    "5\n",
    "Magnitude of Feature 2= \n",
    "1 \n",
    "2\n",
    " +2 \n",
    "2\n",
    " \n",
    "​\n",
    " = \n",
    "1+4\n",
    "​\n",
    " = \n",
    "5\n",
    "​\n",
    " \n",
    "\n",
    "Divide each element of the feature vector by its magnitude:\n",
    "Unit Vector of Feature 1\n",
    "=\n",
    "[\n",
    "3\n",
    "5\n",
    ",\n",
    "4\n",
    "5\n",
    "]\n",
    "Unit Vector of Feature 1=[ \n",
    "5\n",
    "3\n",
    "​\n",
    " , \n",
    "5\n",
    "4\n",
    "​\n",
    " ]\n",
    "\n",
    "Unit Vector of Feature 2\n",
    "=\n",
    "[\n",
    "1\n",
    "5\n",
    ",\n",
    "2\n",
    "5\n",
    "]\n",
    "Unit Vector of Feature 2=[ \n",
    "5\n",
    "​\n",
    " \n",
    "1\n",
    "​\n",
    " , \n",
    "5\n",
    "​\n",
    " \n",
    "2\n",
    "​\n",
    " ]\n",
    "\n",
    "After applying the Unit Vector technique, the feature vectors are scaled to have a magnitude of 1. This normalization technique is particularly useful in scenarios where the direction of the feature vectors is important rather than their absolute values.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baaf48d-0f76-4f05-bab8-b0301220d12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a747ef7-de8e-473a-a3a7-9338e2a78ec5",
   "metadata": {},
   "source": [
    "Data Standardization: PCA often begins with standardizing the data to have a mean of 0 and a standard deviation of 1 across each feature. This step is crucial for ensuring that all features contribute equally to the analysis.\n",
    "\n",
    "Covariance Matrix Computation: PCA calculates the covariance matrix of the standardized data. The covariance matrix represents the relationships between different features.\n",
    "\n",
    "Eigenvalue Decomposition: PCA then performs eigenvalue decomposition or singular value decomposition (SVD) on the covariance matrix to obtain the eigenvectors and eigenvalues. Eigenvectors represent the directions (principal components) of maximum variance in the data, and eigenvalues represent the magnitude of variance along these directions.\n",
    "\n",
    "Selecting Principal Components: PCA ranks the eigenvectors based on their corresponding eigenvalues. The eigenvectors with the highest eigenvalues (explained variance) are considered the principal components. Typically, only a subset of the principal components with the highest variance is retained to reduce dimensionality.\n",
    "\n",
    "Projection: Finally, PCA projects the original data onto the selected principal components, resulting in a lower-dimensional representation of the data.\n",
    "\n",
    "PCA is used in dimensionality reduction to address the curse of dimensionality, where high-dimensional datasets may suffer from increased computational complexity, overfitting, and decreased model interpretability. By reducing the number of dimensions while retaining most of the variance, PCA simplifies the dataset while preserving its essential structure.\n",
    "\n",
    "Example:\n",
    "Suppose we have a dataset with three features: height, weight, and age, representing individuals. We want to reduce the dimensionality of the dataset using PCA.\n",
    "\n",
    "We start by standardizing the data.\n",
    "We compute the covariance matrix.\n",
    "We perform eigenvalue decomposition to obtain eigenvectors and eigenvalues.\n",
    "We select the top two eigenvectors based on their corresponding eigenvalues as the principal components.\n",
    "We project the original data onto these two principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2146f476-1d56-42fa-86e5-91d0fafa5048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76f966ea-7f39-4545-9c95-9b31422ed718",
   "metadata": {},
   "source": [
    "Dimensionality Reduction: PCA is primarily used to reduce the dimensionality of high-dimensional datasets by transforming them into a lower-dimensional space while preserving most of the variance. In this context, the original features serve as input, and the principal components (derived features) serve as output.\n",
    "\n",
    "Feature Selection: PCA implicitly performs feature selection by selecting the principal components with the highest variance. These principal components represent combinations of the original features that contribute the most to the variance in the data. By retaining only a subset of the principal components, PCA effectively selects the most informative features while discarding redundant or noisy ones.\n",
    "\n",
    "Information Compression: PCA compresses the information contained in the original features into a smaller set of principal components. These principal components capture the essential structure of the data, allowing for efficient storage and processing of the dataset.\n",
    "\n",
    "Example:\n",
    "Let's consider an example where we have a dataset containing multiple features representing images of handwritten digits. Each image is represented by a high-dimensional vector of pixel values. We want to extract a smaller set of features that capture the essential characteristics of the images for classification purposes.\n",
    "\n",
    "Data Preprocessing: We preprocess the image data, such as standardizing pixel values.\n",
    "\n",
    "PCA: We apply PCA to the preprocessed dataset to extract a set of principal components. These principal components represent patterns or features in the images that contribute the most to the variance.\n",
    "\n",
    "Dimensionality Reduction: We select a subset of the principal components that capture a significant portion of the variance in the data. For example, we might choose the top 50 principal components out of 1000.\n",
    "\n",
    "Feature Extraction: The selected principal components serve as the extracted features for the images. Each image is now represented by a reduced-dimensional feature vector, which retains most of the information while eliminating noise and redundancy.\n",
    "\n",
    "Classification: We can use the extracted features as input to a classification algorithm, such as a support vector machine (SVM) or a neural network, to classify the images into different digit classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb924668-9ea0-45cc-9181-2ae77145f315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a97083d7-0a6e-4c76-b74c-5660477abb38",
   "metadata": {},
   "source": [
    "Understand the Data: First, you need to understand the dataset and the range of values for each feature. In this case, you have features such as price, rating, and delivery time.\n",
    "\n",
    "Apply Min-Max Scaling: Min-Max scaling involves scaling the features to a specific range, typically between 0 and 1. Here's how you can apply Min-Max scaling to each feature:\n",
    "\n",
    "For each feature (e.g., price, rating, delivery time), calculate the minimum and maximum values.\n",
    "\n",
    "Apply the Min-Max scaling formula to each value of the feature using the following formula:\n",
    "\n",
    "�\n",
    "scaled\n",
    "=\n",
    "�\n",
    "−\n",
    "�\n",
    "min\n",
    "�\n",
    "max\n",
    "−\n",
    "�\n",
    "min\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "X \n",
    "max\n",
    "​\n",
    " −X \n",
    "min\n",
    "​\n",
    " \n",
    "X−X \n",
    "min\n",
    "​\n",
    " \n",
    "​\n",
    " \n",
    "\n",
    "where:\n",
    "\n",
    "�\n",
    "X is the original value of the feature.\n",
    "�\n",
    "min\n",
    "X \n",
    "min\n",
    "​\n",
    "  is the minimum value of the feature.\n",
    "�\n",
    "max\n",
    "X \n",
    "max\n",
    "​\n",
    "  is the maximum value of the feature.\n",
    "Normalization: After applying Min-Max scaling, all features will be scaled to a range between 0 and 1. This normalization ensures that each feature contributes equally to the analysis, regardless of its original scale.\n",
    "\n",
    "Data Analysis and Modeling: Once the data is preprocessed using Min-Max scaling, you can proceed with data analysis and modeling for building the recommendation system. You can use various techniques such as collaborative filtering, content-based filtering, or hybrid approaches to generate personalized recommendations for users based on their preferences and past behavior.\n",
    "\n",
    "Evaluation and Optimization: After building the recommendation system, evaluate its performance using appropriate metrics such as precision, recall, or mean average precision. Iterate on the model and fine-tune parameters to optimize its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d546f71-b001-4e26-9e85-66ce0e750d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df759629-ea80-4eb0-9754-278b8844cf58",
   "metadata": {},
   "source": [
    "Data Preprocessing:\n",
    "\n",
    "Standardize the data: Ensure that all features have a mean of 0 and a standard deviation of 1. Standardization is essential for PCA as it prevents features with larger scales from dominating the analysis.\n",
    "Handle missing values: Impute missing values if any, using appropriate techniques such as mean imputation or interpolation.\n",
    "Apply PCA:\n",
    "\n",
    "Calculate the covariance matrix: Compute the covariance matrix of the standardized data. The covariance matrix represents the relationships between different features.\n",
    "Perform eigenvalue decomposition: Decompose the covariance matrix into its eigenvectors and eigenvalues. Eigenvectors represent the directions of maximum variance in the data, and eigenvalues represent the magnitude of variance along these directions.\n",
    "Select principal components: Rank the eigenvectors based on their corresponding eigenvalues. Choose the top \n",
    "�\n",
    "k eigenvectors (principal components) that capture most of the variance in the data. The number of principal components, \n",
    "�\n",
    "k, is determined based on the desired level of dimensionality reduction or the amount of variance to be retained (e.g., retaining 95% of the variance).\n",
    "Project the data onto the selected principal components: Multiply the standardized data matrix by the selected principal components to obtain the lower-dimensional representation of the data.\n",
    "Dimensionality Reduction:\n",
    "\n",
    "The lower-dimensional representation obtained after projecting the data onto the selected principal components effectively reduces the dimensionality of the dataset while retaining most of the variance. This reduced representation contains fewer features (principal components) compared to the original dataset, making it computationally efficient for modeling while preserving essential information.\n",
    "Modeling:\n",
    "\n",
    "Use the reduced-dimensional dataset obtained after PCA as input to train predictive models for stock price prediction. Various machine learning algorithms such as regression, neural networks, or ensemble methods can be used for modeling.\n",
    "Evaluation and Optimization:\n",
    "\n",
    "Evaluate the performance of the predictive models using appropriate metrics such as mean squared error (MSE), mean absolute error (MAE), or R-squared. Optimize the models by fine-tuning hyperparameters or exploring different feature combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550103ff-0805-49d7-bc96-f8a9d7208819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddeb210e-89dc-4211-acf4-a4fefab58d1d",
   "metadata": {},
   "source": [
    "Find the minimum and maximum values in the dataset.\n",
    "�\n",
    "min\n",
    "=\n",
    "1\n",
    "X \n",
    "min\n",
    "​\n",
    " =1\n",
    "\n",
    "�\n",
    "max\n",
    "=\n",
    "20\n",
    "X \n",
    "max\n",
    "​\n",
    " =20\n",
    "\n",
    "Apply Min-Max scaling formula to each value in the dataset.\n",
    "�\n",
    "scaled\n",
    "=\n",
    "�\n",
    "−\n",
    "�\n",
    "min\n",
    "�\n",
    "max\n",
    "−\n",
    "�\n",
    "min\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "X \n",
    "max\n",
    "​\n",
    " −X \n",
    "min\n",
    "​\n",
    " \n",
    "X−X \n",
    "min\n",
    "​\n",
    " \n",
    "​\n",
    " \n",
    "\n",
    "After applying the Min-Max scaling formula, adjust the scaled values to the desired range of -1 to 1.\n",
    "�\n",
    "scaled_adjusted\n",
    "=\n",
    "2\n",
    "×\n",
    "�\n",
    "scaled\n",
    "−\n",
    "1\n",
    "X \n",
    "scaled_adjusted\n",
    "​\n",
    " =2×X \n",
    "scaled\n",
    "​\n",
    " −1\n",
    "\n",
    "Let's calculate:\n",
    "\n",
    "For \n",
    "�\n",
    "=\n",
    "1\n",
    "X=1:\n",
    "�\n",
    "scaled\n",
    "=\n",
    "1\n",
    "−\n",
    "1\n",
    "20\n",
    "−\n",
    "1\n",
    "=\n",
    "0\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "20−1\n",
    "1−1\n",
    "​\n",
    " =0\n",
    "�\n",
    "scaled_adjusted\n",
    "=\n",
    "2\n",
    "×\n",
    "0\n",
    "−\n",
    "1\n",
    "=\n",
    "−\n",
    "1\n",
    "X \n",
    "scaled_adjusted\n",
    "​\n",
    " =2×0−1=−1\n",
    "\n",
    "For \n",
    "�\n",
    "=\n",
    "5\n",
    "X=5:\n",
    "�\n",
    "scaled\n",
    "=\n",
    "5\n",
    "−\n",
    "1\n",
    "20\n",
    "−\n",
    "1\n",
    "=\n",
    "4\n",
    "19\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "20−1\n",
    "5−1\n",
    "​\n",
    " = \n",
    "19\n",
    "4\n",
    "​\n",
    " \n",
    "�\n",
    "scaled_adjusted\n",
    "=\n",
    "2\n",
    "×\n",
    "4\n",
    "19\n",
    "−\n",
    "1\n",
    "X \n",
    "scaled_adjusted\n",
    "​\n",
    " =2× \n",
    "19\n",
    "4\n",
    "​\n",
    " −1\n",
    "\n",
    "For \n",
    "�\n",
    "=\n",
    "10\n",
    "X=10:\n",
    "�\n",
    "scaled\n",
    "=\n",
    "10\n",
    "−\n",
    "1\n",
    "20\n",
    "−\n",
    "1\n",
    "=\n",
    "9\n",
    "19\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "20−1\n",
    "10−1\n",
    "​\n",
    " = \n",
    "19\n",
    "9\n",
    "​\n",
    " \n",
    "�\n",
    "scaled_adjusted\n",
    "=\n",
    "2\n",
    "×\n",
    "9\n",
    "19\n",
    "−\n",
    "1\n",
    "X \n",
    "scaled_adjusted\n",
    "​\n",
    " =2× \n",
    "19\n",
    "9\n",
    "​\n",
    " −1\n",
    "\n",
    "For \n",
    "�\n",
    "=\n",
    "15\n",
    "X=15:\n",
    "�\n",
    "scaled\n",
    "=\n",
    "15\n",
    "−\n",
    "1\n",
    "20\n",
    "−\n",
    "1\n",
    "=\n",
    "14\n",
    "19\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "20−1\n",
    "15−1\n",
    "​\n",
    " = \n",
    "19\n",
    "14\n",
    "​\n",
    " \n",
    "�\n",
    "scaled_adjusted\n",
    "=\n",
    "2\n",
    "×\n",
    "14\n",
    "19\n",
    "−\n",
    "1\n",
    "X \n",
    "scaled_adjusted\n",
    "​\n",
    " =2× \n",
    "19\n",
    "14\n",
    "​\n",
    " −1\n",
    "\n",
    "For \n",
    "�\n",
    "=\n",
    "20\n",
    "X=20:\n",
    "�\n",
    "scaled\n",
    "=\n",
    "20\n",
    "−\n",
    "1\n",
    "20\n",
    "−\n",
    "1\n",
    "=\n",
    "1\n",
    "X \n",
    "scaled\n",
    "​\n",
    " = \n",
    "20−1\n",
    "20−1\n",
    "​\n",
    " =1\n",
    "�\n",
    "scaled_adjusted\n",
    "=\n",
    "2\n",
    "×\n",
    "1\n",
    "−\n",
    "1\n",
    "=\n",
    "1\n",
    "X \n",
    "scaled_adjusted\n",
    "​\n",
    " =2×1−1=1\n",
    "\n",
    "So, after Min-Max scaling and adjusting to the range of -1 to 1, the dataset becomes:\n",
    "\n",
    "Scaled Dataset\n",
    "=\n",
    "[\n",
    "−\n",
    "1\n",
    ",\n",
    "−\n",
    "10\n",
    "19\n",
    ",\n",
    "−\n",
    "2\n",
    "19\n",
    ",\n",
    "8\n",
    "19\n",
    ",\n",
    "1\n",
    "]\n",
    "Scaled Dataset=[−1,− \n",
    "19\n",
    "10\n",
    "​\n",
    " ,− \n",
    "19\n",
    "2\n",
    "​\n",
    " , \n",
    "19\n",
    "8\n",
    "​\n",
    " ,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcdbb08-94ce-44a2-91f3-b9eaab3def57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
